<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>Caffe_notes - Yilin's Wiki</title>
    <meta name="keywords" content=""/>
    <meta name="description" content=""/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          <!--$表示行内元素，$$表示块状元素 -->
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!--Mermaid流程图-->
    <script src="https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#deep_learning_tools">deep_learning_tools</a>&nbsp;&#187;&nbsp;Caffe_notes
    <span class="updated">Page Updated&nbsp;
      2019-07-09 01:01
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">Caffe_notes</div>

  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#caffe">Caffe 依赖库</a><ul>
<li><a href="#protobuffer">ProtoBuffer</a></li>
<li><a href="#boost">Boost</a></li>
<li><a href="#gflags">GFLAGS</a></li>
<li><a href="#glog">GLOG</a></li>
<li><a href="#blas">BLAS</a></li>
<li><a href="#hdf5">HDF5</a></li>
<li><a href="#opencv">OpenCV</a></li>
<li><a href="#lmdb-leveldb">LMDB 和 LevelDB</a></li>
<li><a href="#snappy">Snappy</a></li>
</ul>
</li>
<li><a href="#caffe_1">Caffe 代码阅读建议</a></li>
<li><a href="#caffe_2">Caffe 前馈神经网络例子</a><ul>
<li><a href="#_1">卷积层</a></li>
<li><a href="#_2">全连接层</a></li>
<li><a href="#_3">激活函数</a></li>
</ul>
</li>
<li><a href="#caffe_3">Caffe 中的数据结构</a><ul>
<li><a href="#blob">Blob</a></li>
<li><a href="#layer">Layer</a></li>
<li><a href="#net">Net</a></li>
</ul>
</li>
<li><a href="#caffe-io">Caffe IO 模块</a><ul>
<li><a href="#_4">数据读取层</a></li>
<li><a href="#_5">数据转换器</a></li>
</ul>
</li>
<li><a href="#caffe_4">Caffe 模型</a><ul>
<li><a href="#caffe-model-zoo">Caffe Model Zoo</a></li>
</ul>
</li>
<li><a href="#caffe_5">Caffe 前向传播计算</a></li>
<li><a href="#caffe_6">Caffe 反向传播计算</a></li>
<li><a href="#caffe_7">Caffe 最优化求解</a><ul>
<li><a href="#caffe_8">Caffe 中的求解器</a></li>
</ul>
</li>
<li><a href="#caffe_9">Caffe 工具</a></li>
<li><a href="#caffe-gpu">Caffe GPU 加速</a></li>
<li><a href="#caffe_10">Caffe 可视化</a></li>
</ul>
</div>
<blockquote>
<p><a href="http://caffe.berkeleyvision.org/">Caffe</a>，全称Convolutional Architecture for Fast Feature Embedding，是一个兼具表达性、速度和思维模块化的深度学习框架。由伯克利人工智能研究小组和伯克利视觉和学习中心开发。虽然其内核是用C++编写的，但Caffe有Python和Matlab 相关接口。Caffe支持多种类型的深度学习架构，面向图像分类和图像分割，还支持CNN、RCNN、LSTM和全连接神经网络设计。Caffe支持基于GPU和CPU的加速计算内核库，如NVIDIA cuDNN和Intel MKL。</p>
</blockquote>
<hr />
<h2 id="caffe">Caffe 依赖库</h2>
<h3 id="protobuffer">ProtoBuffer</h3>
<p>Google 开发的一种内存与非易失存储介质（如硬盘文件）交换的协议接口。</p>
<p>Caffe 用 PB 来保存 <em>权值</em> 和 <em>模型参数</em> 。</p>
<p>使用统一的参数描述文件 (.proto)，根据 .proto 文件生成协议细节代码。</p>
<h3 id="boost">Boost</h3>
<p>强大的开源 C++ 库，C++ 标准库的后备。</p>
<p>Caffe 使用了 Boost 的智能指针，利用 Boost Python 实现了 <code>pycaffe</code> 。</p>
<h3 id="gflags">GFLAGS</h3>
<p>Google 的开源命令行参数解析库。</p>
<h3 id="glog">GLOG</h3>
<p>Google 的开源日志库。</p>
<h3 id="blas">BLAS</h3>
<p>CPU 端矩阵计算库。</p>
<h3 id="hdf5">HDF5</h3>
<p>HDF, Hierarchical Data File，一种数据格式。实现高效的存储、分发。</p>
<h3 id="opencv">OpenCV</h3>
<p>图像读写、预处理操作。Caffe 实际上用到 OpenCV 的部分很少。</p>
<h3 id="lmdb-leveldb">LMDB 和 LevelDB</h3>
<p>LMDB -- Lightning Memory-Mapped Database Manager，在 Caffe 中提供数据管理功能，将图像、二进制数据等统一用 key-value 形式存储，便于 <code>DataLayer</code> 读取。</p>
<p>LevelDB 是老版本 Caffe 使用的数据库，目前大部分例子使用 LMDB。</p>
<h3 id="snappy">Snappy</h3>
<p>一个用来压缩和解压缩的 C++ 库，比 zlib 快，但是文件要更大。</p>
<hr />
<h2 id="caffe_1">Caffe 代码阅读建议</h2>
<ul>
<li>先看 <code>src/caffe/proto/caffe.proto</code>，了解基本数据结构内存对象与磁盘文件的一一映射关系；</li>
<li>看头文件，了解每个模块大概的作用；</li>
<li>有针对性地去看 <code>cpp</code> 和 <code>cu</code> 文件；</li>
<li>一般按需求去派生新的类即可，如继承 <code>ConvolutionLayer</code> 去实现自己的卷积层</li>
<li>根据 <code>tools</code> 下面的工具去自己修改、编写新工具</li>
</ul>
<p>利用 grep 来迅速定位代码：</p>
<div class="hlcode"><pre><span></span><span class="n">grep</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">H</span> <span class="o">-</span><span class="n">R</span> <span class="ss">&quot;xxx&quot;</span> <span class="o">*</span>

<span class="o">#</span> <span class="o">-</span><span class="n">n</span> <span class="err">显示行号</span>
<span class="o">#</span> <span class="o">-</span><span class="n">H</span> <span class="err">显示文件名</span>
<span class="o">#</span> <span class="o">-</span><span class="n">R</span> <span class="err">递归查找子目录</span>
</pre></div>


<hr />
<h2 id="caffe_2">Caffe 前馈神经网络例子</h2>
<ul>
<li>输入层为二维图像数据，前几层均为卷积层，提取特征；最后两层为全连接层，类似于多层感知机，用于对前面提供的特征进行分类。</li>
<li>卷积层和全连接层统称为权值层，它们都有需要学习出的参数（权值）。</li>
</ul>
<h3 id="_1">卷积层</h3>
<ul>
<li>多个通道，每个通道按照二维卷积方式计算；</li>
<li>多个通道与多个卷积核分别进行卷积，得到多通道输出，需要“合并”为一个通道；</li>
<li>假设卷积层有 <code>L</code> 个输出通道和 <code>K</code> 个输入通道，需要 <code>L*K</code> 个卷积核实现通道数转换；</li>
<li>假设卷积核的大小为 <code>I*J</code>，每个输出通道的 feature map （特征图）大小均为 <code>M*N</code>，则该层每个样本做一次前向传播时的计算量为： <code>Calculations = I * J * M * N * K * L</code></li>
<li>在 mnist/lenet_train_val.prototxt 中找到卷积核参数描述：</li>
</ul>
<div class="hlcode"><pre><span></span><span class="n">convolution_param</span> <span class="err">{</span>
    <span class="n">num_output</span><span class="p">:</span> <span class="mi">50</span>  <span class="o">//</span> <span class="err">对应上面的</span> <span class="n">L</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">5</span>  <span class="o">//</span> <span class="err">对应上面的</span> <span class="n">I</span> <span class="n">J</span>
    <span class="n">stride</span><span class="p">:</span> <span class="mi">1</span>  <span class="o">//</span> <span class="err">滑动窗口每次移动的步长</span>
    <span class="p">......</span>
<span class="err">}</span>
</pre></div>


<ul>
<li>当前层的学习参数数量： <code>Params = I * J * K * L</code></li>
<li>CPR (Calculations to Parameters Ratio): <code>CPR = M * N</code></li>
<li>卷积层的特征图的尺寸越大，CPR 越大，参数重复利用率越高</li>
</ul>
<h3 id="_2">全连接层</h3>
<ul>
<li>CNN 之前，深度学习网络计算模型都是全连接形式的 DNN，每个节点与相邻层所有节点都有连接关系；</li>
<li>全连接层主要计算为“矩阵与向量乘积”： $y = W * x$，这里 x 为输入向量，y 为输出向量，W 为权值矩阵；</li>
<li>全连接层中，直接将前一层输出展开为一维向量（不考虑 batch 的维度）：</li>
</ul>
<div class="hlcode"><pre><span></span><span class="n">inner_product_param</span> <span class="err">{</span>
    <span class="n">num_output</span><span class="p">:</span> <span class="mi">500</span>  <span class="o">//</span> <span class="err">输出向量维数</span>
    <span class="p">......</span>
<span class="err">}</span>
</pre></div>


<ul>
<li>设 W 矩阵大小为 $V * D$ ，全连接层计算量与参数量都是 O($V * D$)，因此全连接层的 CPR 与输入、输出维度无关；</li>
<li>上面考虑的是全连接层处理单个样本的情况。如果将一批样本（B个）逐列拼接成输入矩阵 X，此时参数量不变，但计算量提高了B倍，权值矩阵在多样本之间实现了重用；</li>
<li>Caffe 中全连接层的实现可以查找 <code>InnerProductLayer</code>；</li>
<li>卷积层相比全连接层，实现了权值共享，这是降低参数量的重要举措，同时卷积层的局部连接特性也大幅减少了参数量。</li>
<li>这对这些特点，大部分 CNN 网络中前几层卷积层参数量占比小，计算量占比大；后几层全连接层参数量占比大，计算量占比小。因此，加速计算优化时，重点在于卷积层；参数优化、权值裁剪时，重点在于全连接层。</li>
</ul>
<h3 id="_3">激活函数</h3>
<ul>
<li>Activation function 或 Squashing function；</li>
<li>非线性单元，多层神经网络如果不引入非线性单元，总可以化简为单层网络；</li>
<li>常见的激活函数有 sigmoid, tanh, ReLU 等；</li>
<li>DNN 训练会遇到梯度消失问题（Gradient Vanishing Problem），这在使用 sigmoid, tanh 等饱和激活函数时尤为严重；</li>
<li>神经网络进行误差反向传播时，<strong>各层都要乘以激活函数的一阶导数</strong>，梯度每传递一层都要衰减一次，网络层数较多时，梯度会不停衰减直至消失；这使得训练时收敛速度非常慢；</li>
<li>使用 ReLU 这类非饱和激活函数训练收敛速度会快很多；</li>
<li>Caffe 中激活函数相关的 <code>Layer</code> 在 <code>include/caffe/neural_layers.hpp</code> 中，可以称为<strong>非线性层</strong>；</li>
<li>非线性层的共同特点就是对前一层 <code>blob</code> 中的数值逐一进行非线性变换，再放回原 <code>blob</code> 中。</li>
</ul>
<hr />
<h2 id="caffe_3">Caffe 中的数据结构</h2>
<ul>
<li>Caffe 中一个 CNN 模型用 Net 表示，一个 Net 对应一个 prototxt 文件</li>
<li>一个 Net 由多个 Layer 堆叠成</li>
<li>Layer 由多个 Blob 组成</li>
</ul>
<h3 id="blob">Blob</h3>
<p>一个 Blob 在内存中表示 4 维数组：(width, height, channels, num)</p>
<p>width 和 height 表示图像宽高，channels 表示通道数，num 表示第几张图</p>
<p>Blob 用于存储数据或权值、权值增量</p>
<p>在进行网络计算时，每一层的输入、输出都需要 Blob 对象缓冲。</p>
<p>Blob 对象有 <code>ToProto()</code>, <code>FromProto()</code> 方法，可以实现内存与磁盘数据的交互。（通过 BlobProto 类型读写）</p>
<p>通过 Blob 来看使用 PB 的好处：（为什么不在 C++ 中直接定义结构体）</p>
<p>－ 结构体的序列化／反序列化需要额外编程<br />
－ 不同结构体难以做到标准统一</p>
<p>Blob 是一个模版类，其中封装了 SyncedMemory 类对象（存放data、diff）</p>
<h3 id="layer">Layer</h3>
<p>Layer 是 Caffe 的基本计算单元，至少有一个输入 Blob 和一个输出 Blob</p>
<p>输入是 bottom</p>
<p>输出是 top</p>
<p>一些 Layer 有 weight 和 bias</p>
<p>Layer 有两个运算方向：前向传播和反向传播</p>
<p>－ 前向传播会对输入 Blob 进行某种处理（有 weight 和 bias 的层会利用这些信息对输入进行处理），得到输出 Blob<br />
－ 反向传播计算对输出 Blob 的 diff 进行某种处理，得到输入 Blob 的 diff（ weight 和 bias 的 diff 可能会被用到）</p>
<p>下面四个函数会在各个 Layer 的派生类中经常看到：</p>
<ul>
<li>Forward_cpu()</li>
<li>Backward_cpu()</li>
<li>Forward_gpu()</li>
<li>Backward_gpu()</li>
</ul>
<p>Layer 这个类是一个虚基类，大部分函数都没有实现</p>
<h3 id="net">Net</h3>
<p>Net 在 Caffe 中代表一个完整的 CNN 模型，它包含若干 Layer 实例。LeNet，AlexNet 等这些网络用 prototxt 描述，在 Caffe 中就是一个 Net 对象</p>
<p>Net 中既有 Layer 对象，又有 Blob 对象。</p>
<p>Blob 对象用于存放每个 Layer 输入／输出中间结果，Layer 对象则根据 Net 描述对指定的输入 Blob 进行某些计算处理；输入 Blob 和输出 Blob 可能为同一个</p>
<p>注意：Blob 名字和 Layer 名字相同并不代表它们有直接关系</p>
<p>有两种 blob，分别为数据 blob 和权值 blob；深度学习的目的就是不断从“数据”中获取知识，存储到“模型”中，应用于后来的“数据”。</p>
<p>Net -&gt; Layer -&gt; Blob</p>
<p>Blob 提供了数据容器的机制，Layer 通过不通的策略使用它们，实现多元化的计算处理过程，同时提供深度学习的各种基本算法（卷积，池化，损失函数计算等）；Net 则利用 Layer 的这些机制，组合为一个完整的深度学习模型。</p>
<hr />
<h2 id="caffe-io">Caffe IO 模块</h2>
<p>之前的例子：将数据转换为 LMDB 格式，训练网络时由数据读取层（DataLayer）不断从 LMDB 读取数据，送入后续卷积、池化等层。</p>
<h3 id="_4">数据读取层</h3>
<p>除了从 LMDB 、LEVELDB 等读取外，也支持从原始图像直接读取（ImageDataLayer）</p>
<ul>
<li>batch_size: 一个批量数据包含的图片数目</li>
<li>rand_skip: 随机跳过若干图片</li>
</ul>
<p>数据读取层代码： include/caffe/data_layers.hpp</p>
<h3 id="_5">数据转换器</h3>
<p>Data Transformer</p>
<p>一些预处理方法：随机镜像、随机切块、去均值、灰度变换等。</p>
<p>其他数据层：memory_data_layer, window_data_layer, hdf5_data_layer</p>
<hr />
<h2 id="caffe_4">Caffe 模型</h2>
<p>一个完整的深度学习系统：数据 ＋ 模型</p>
<p>一个深度学习模型通常有三部分：</p>
<ul>
<li>可学习参数（Learnable Parameter）:训练参数，神经网络权值，其数值由模型初始化参数、误差反向传播控制，一般不可人工干预</li>
<li>结构参数（Archetecture Parameter）:卷积层、全连接层、下采样层数目，卷积核数目，卷积核大小等用来描述网络结构的参数，训练前事先设定好；注意，训练阶段和预测阶段网络结构参数很可能不同</li>
<li>训练超参数（Hyper-Parameter）:用来控制网络训练收敛的参数，往往调参调的就是这个；预测网络不需要该参数</li>
</ul>
<p>可学习参数在内存中使用 Blob 对象保持，必要时以二进制保存在磁盘上（.caffemodel），便于 finetune 、共享、与性能评估（benchmark）</p>
<p>结构参数通过 ProtoBuffer 文本格式描述，通过该描述文件构建 Net 对象、Layer 对象，形成 DAG（有向无环图），在 Layer 与 Layer 之间、Net 输入与输出之间均为持有数据和中间结果的 Blob 对象。</p>
<p>训练超参数也是通过 prototxt 保存，训练时利用该描述文件构建 Solver，该对象按照一定规则在训练网络时自动调节这些超参数</p>
<h3 id="caffe-model-zoo">Caffe Model Zoo</h3>
<p>对于规模很大的模型，是否每次都需要从头训练？</p>
<p>Caffe Model Zoo 提供了一个分享模型的平台</p>
<hr />
<h2 id="caffe_5">Caffe 前向传播计算</h2>
<p>CNN 训练时一般包括了前向传播和反向传播两个阶段</p>
<p>前向传播阶段，数据从数据读取层出发，经过若干处理层，达到最后一层（损失层或特征层）</p>
<p>网络中的权值在 FP 过程中不发生变化，可以看作常量。</p>
<p>网络路径是一个有向无环图</p>
<p>Caffe 中 FP 通过 Net ＋ Layer 组合完成，中间结果和最终结果通过 Blob 承载。</p>
<p>前向传播：从输入层到输出层，一层一层利用已有权值来更新 loss 和最终输出</p>
<p>反向传播：从输出层到输入层，从输出层总误差开始一层一层更新权值</p>
<p>前向传播在训练和预测中都要用到</p>
<p>反向传播只在训练时用到</p>
<p>Loss Layer 是 CNN 的终点，接受两个 Blob 作为输入，其中一个为 CNN 的预测值，另一个是真实标签。 损失层将这两个输入进行一系列运算，计算损失函数值 L(\theta), \theta 是模型的权值，训练的目标就是得到最优权值，使得 Loss 最小。</p>
<p>损失函数是在前向传播计算中得到的，同时也是反向传播的起点</p>
<p>Caffe 中实现了多种损失层，分别用于不同场合，其中 SoftmaxWithLossLayer 实现了 Softmax ＋ 交叉熵损失函数计算过程</p>
<p>Caffe 训练时的的问题“为什么 Loss 总在 6.9 左右？”，因为 -ln(0.001) = 6.907755... （ImageNet-1000 分类问题初始状态为均匀分布，每个类别的分类概率为 0.001 ）。这说明没有收敛的迹象，需要调大学习率或者修改权值初始化方式</p>
<p>一个 Blob 中有 data 和 diff 两部分数据，数据读取层提供了 data， 损失层提供了 diff</p>
<hr />
<h2 id="caffe_6">Caffe 反向传播计算</h2>
<p>从 Loss 层向数据层方向传播 diff ，更新权值</p>
<hr />
<h2 id="caffe_7">Caffe 最优化求解</h2>
<p>Caffe 求解器的职责：</p>
<ul>
<li>负责记录优化过程，创建用于学习的训练网络和用于评估学习效果的测试网络</li>
<li>调用 Forward －》 调用 Backward －》 更新权值，反复迭代优化模型</li>
<li>周期性评估测试网络</li>
<li>在优化中为模型、求解器状态打快照</li>
</ul>
<p>Caffe 求解器每次迭代中做的事情：</p>
<ul>
<li>调用 Net 的前向传播函数来计算输出和损失函数</li>
<li>调用 Net 的反向传播函数来计算梯度</li>
<li>根据求解器方法，将梯度转换为权值增量</li>
<li>根据学习率、历史权值、所用方法更新求解器状态</li>
</ul>
<h3 id="caffe_8">Caffe 中的求解器</h3>
<ul>
<li>SGD，随机梯度下降</li>
<li>AdaDelta</li>
<li>ADAGRAD，自适应梯度</li>
<li>Adam</li>
<li>Nesterov，加速梯度法</li>
<li>RMSprop</li>
</ul>
<p>求解器方法的重点在于最小化损失函数的全局优化问题</p>
<p>SGD算法，利用负梯度和权值更新历史的线性组合更新权值 W</p>
<ul>
<li>SGD 算法里的超参数：</li>
<li>lr，学习率，负梯度的权重</li>
<li>momentum，权值更新历史的权重</li>
</ul>
<hr />
<h2 id="caffe_9">Caffe 工具</h2>
<p>tools/ 下有一些利用 caffe 编写的程序，如训练、预测、预处理等</p>
<hr />
<h2 id="caffe-gpu">Caffe GPU 加速</h2>
<p>略，CUDA ＋ cuDNN</p>
<hr />
<h2 id="caffe_10">Caffe 可视化</h2>
<ul>
<li>原始数据可视化</li>
<li>网络可视化（利用 draw_net.py 来画）</li>
<li>网络权值可视化：通常第一个卷积层是最容易解释的，良好训练的网络权值通常表现为美观、光滑的滤波器</li>
<li>如果网络权值可视化中出现图案相关性高、缺乏结构性等情况，可能说明训练结果不好</li>
</ul>
<p>训练脚本时这么写：</p>
<div class="hlcode"><pre><span></span><span class="n">xxx</span><span class="p">.</span><span class="n">sh</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> <span class="n">xxx</span><span class="p">.</span><span class="n">log</span> <span class="o">&amp;</span>
<span class="o">#</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> <span class="err">表示</span> <span class="k">stdout</span> <span class="err">和</span> <span class="n">stderr</span> <span class="err">都重定向到</span> <span class="n">xxx</span><span class="p">.</span><span class="n">log</span>
<span class="o">#</span> <span class="err">最后的</span> <span class="o">&amp;</span> <span class="err">表示在后台运行</span>
</pre></div>


<p>可以用 tail -f xxx.log 观察 log 文件的更新</p>
<p>可以使用如下命令提取 log 文件中的 loss 值：</p>
<div class="hlcode"><pre><span></span><span class="n">cat</span> <span class="n">xxx</span><span class="p">.</span><span class="n">log</span> <span class="o">|</span> <span class="n">grep</span> <span class="ss">&quot;Train net output&quot;</span> <span class="o">|</span> <span class="n">awk</span> <span class="s1">&#39;{print $11}&#39;</span>
</pre></div>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2019 Yilin Gui.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2019-12-03 00:35:45</p>
      </span>
    </div>

    
    
  </body>
</html>
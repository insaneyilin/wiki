<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>Caffe_notes - Yilin's Wiki</title>
    <meta name="keywords" content=""/>
    <meta name="description" content=""/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          <!--$表示行内元素，$$表示块状元素 -->
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#deep_learning_tools">deep_learning_tools</a>&nbsp;&#187;&nbsp;Caffe_notes
    <span class="updated">Page Updated&nbsp;
      2019-07-09 01:01
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">Caffe_notes</div>

  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#caffe">Caffe 依赖库</a><ul>
<li><a href="#protobuffer">ProtoBuffer</a></li>
<li><a href="#boost">Boost</a></li>
<li><a href="#gflags">GFLAGS</a></li>
<li><a href="#glog">GLOG</a></li>
<li><a href="#blas">BLAS</a></li>
<li><a href="#hdf5">HDF5</a></li>
<li><a href="#opencv">OpenCV</a></li>
<li><a href="#lmdb-leveldb">LMDB 和 LevelDB</a></li>
<li><a href="#snappy">Snappy</a></li>
</ul>
</li>
<li><a href="#caffe_1">Caffe 代码阅读建议</a></li>
<li><a href="#caffe_2">Caffe 前馈神经网络例子</a><ul>
<li><a href="#_1">卷积层</a></li>
<li><a href="#_2">全连接层</a></li>
<li><a href="#_3">激活函数</a></li>
</ul>
</li>
<li><a href="#caffe_3">Caffe 中的数据结构</a></li>
</ul>
</div>
<blockquote>
<p><a href="http://caffe.berkeleyvision.org/">Caffe</a>，全称Convolutional Architecture for Fast Feature Embedding，是一个兼具表达性、速度和思维模块化的深度学习框架。由伯克利人工智能研究小组和伯克利视觉和学习中心开发。虽然其内核是用C++编写的，但Caffe有Python和Matlab 相关接口。Caffe支持多种类型的深度学习架构，面向图像分类和图像分割，还支持CNN、RCNN、LSTM和全连接神经网络设计。Caffe支持基于GPU和CPU的加速计算内核库，如NVIDIA cuDNN和Intel MKL。</p>
</blockquote>
<hr />
<h2 id="caffe">Caffe 依赖库</h2>
<h3 id="protobuffer">ProtoBuffer</h3>
<p>Google 开发的一种内存与非易失存储介质（如硬盘文件）交换的协议接口。</p>
<p>Caffe 用 PB 来保存 <em>权值</em> 和 <em>模型参数</em> 。</p>
<p>使用统一的参数描述文件 (.proto)，根据 .proto 文件生成协议细节代码。</p>
<h3 id="boost">Boost</h3>
<p>强大的开源 C++ 库，C++ 标准库的后备。</p>
<p>Caffe 使用了 Boost 的智能指针，利用 Boost Python 实现了 <code>pycaffe</code> 。</p>
<h3 id="gflags">GFLAGS</h3>
<p>Google 的开源命令行参数解析库。</p>
<h3 id="glog">GLOG</h3>
<p>Google 的开源日志库。</p>
<h3 id="blas">BLAS</h3>
<p>CPU 端矩阵计算库。</p>
<h3 id="hdf5">HDF5</h3>
<p>HDF, Hierarchical Data File，一种数据格式。实现高效的存储、分发。</p>
<h3 id="opencv">OpenCV</h3>
<p>图像读写、预处理操作。Caffe 实际上用到 OpenCV 的部分很少。</p>
<h3 id="lmdb-leveldb">LMDB 和 LevelDB</h3>
<p>LMDB -- Lightning Memory-Mapped Database Manager，在 Caffe 中提供数据管理功能，将图像、二进制数据等统一用 key-value 形式存储，便于 <code>DataLayer</code> 读取。</p>
<p>LevelDB 是老版本 Caffe 使用的数据库，目前大部分例子使用 LMDB。</p>
<h3 id="snappy">Snappy</h3>
<p>一个用来压缩和解压缩的 C++ 库，比 zlib 快，但是文件要更大。</p>
<hr />
<h2 id="caffe_1">Caffe 代码阅读建议</h2>
<ul>
<li>先看 <code>src/caffe/proto/caffe.proto</code>，了解基本数据结构内存对象与磁盘文件的一一映射关系；</li>
<li>看头文件，了解每个模块大概的作用；</li>
<li>有针对性地去看 <code>cpp</code> 和 <code>cu</code> 文件；</li>
<li>一般按需求去派生新的类即可，如继承 <code>ConvolutionLayer</code> 去实现自己的卷积层</li>
<li>根据 <code>tools</code> 下面的工具去自己修改、编写新工具</li>
</ul>
<p>利用 grep 来迅速定位代码：</p>
<div class="hlcode"><pre><span class="n">grep</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">H</span> <span class="o">-</span><span class="n">R</span> <span class="s">&quot;xxx&quot;</span> <span class="o">*</span>

<span class="cp"># -n 显示行号</span>
<span class="cp"># -H 显示文件名</span>
<span class="cp"># -R 递归查找子目录</span>
</pre></div>


<hr />
<h2 id="caffe_2">Caffe 前馈神经网络例子</h2>
<ul>
<li>输入层为二维图像数据，前几层均为卷积层，提取特征；最后两层为全连接层，类似于多层感知机，用于对前面提供的特征进行分类。</li>
<li>卷积层和全连接层统称为权值层，它们都有需要学习出的参数（权值）。</li>
</ul>
<h3 id="_1">卷积层</h3>
<ul>
<li>多个通道，每个通道按照二维卷积方式计算；</li>
<li>多个通道与多个卷积核分别进行卷积，得到多通道输出，需要“合并”为一个通道；</li>
<li>假设卷积层有 <code>L</code> 个输出通道和 <code>K</code> 个输入通道，需要 <code>L*K</code> 个卷积核实现通道数转换；</li>
<li>假设卷积核的大小为 <code>I*J</code>，每个输出通道的 feature map （特征图）大小均为 <code>M*N</code>，则该层每个样本做一次前向传播时的计算量为： <code>Calculations = I * J * M * N * K * L</code></li>
<li>在 mnist/lenet_train_val.prototxt 中找到卷积核参数描述：</li>
</ul>
<div class="hlcode"><pre><span class="n">convolution_param</span> <span class="p">{</span>
    <span class="nl">num_output:</span> <span class="mi">50</span>  <span class="c1">// 对应上面的 L</span>
    <span class="nl">kernel_size:</span> <span class="mi">5</span>  <span class="c1">// 对应上面的 I J</span>
    <span class="nl">stride:</span> <span class="mi">1</span>  <span class="c1">// 滑动窗口每次移动的步长</span>
    <span class="p">......</span>
<span class="p">}</span>
</pre></div>


<ul>
<li>当前层的学习参数数量： <code>Params = I * J * K * L</code></li>
<li>CPR (Calculations to Parameters Ratio): <code>CPR = M * N</code></li>
<li>卷积层的特征图的尺寸越大，CPR 越大，参数重复利用率越高</li>
</ul>
<h3 id="_2">全连接层</h3>
<ul>
<li>CNN 之前，深度学习网络计算模型都是全连接形式的 DNN，每个节点与相邻层所有节点都有连接关系；</li>
<li>全连接层主要计算为“矩阵与向量乘积”： $y = W * x$，这里 x 为输入向量，y 为输出向量，W 为权值矩阵；</li>
<li>全连接层中，直接将前一层输出展开为一维向量（不考虑 batch 的维度）：</li>
</ul>
<div class="hlcode"><pre><span class="n">inner_product_param</span> <span class="p">{</span>
    <span class="nl">num_output:</span> <span class="mi">500</span>  <span class="c1">// 输出向量维数</span>
    <span class="p">......</span>
<span class="p">}</span>
</pre></div>


<ul>
<li>设 W 矩阵大小为 $V * D$ ，全连接层计算量与参数量都是 O($V * D$)，因此全连接层的 CPR 与输入、输出维度无关；</li>
<li>上面考虑的是全连接层处理单个样本的情况。如果将一批样本（B个）逐列拼接成输入矩阵 X，此时参数量不变，但计算量提高了B倍，权值矩阵在多样本之间实现了重用；</li>
<li>Caffe 中全连接层的实现可以查找 <code>InnerProductLayer</code>；</li>
<li>卷积层相比全连接层，实现了权值共享，这是降低参数量的重要举措，同时卷积层的局部连接特性也大幅减少了参数量。</li>
<li>这对这些特点，大部分 CNN 网络中前几层卷积层参数量占比小，计算量占比大；后几层全连接层参数量占比大，计算量占比小。因此，加速计算优化时，重点在于卷积层；参数优化、权值裁剪时，重点在于全连接层。</li>
</ul>
<h3 id="_3">激活函数</h3>
<ul>
<li>Activation function 或 Squashing function；</li>
<li>非线性单元，多层神经网络如果不引入非线性单元，总可以化简为单层网络；</li>
<li>常见的激活函数有 sigmoid, tanh, ReLU 等；</li>
<li>DNN 训练会遇到梯度消失问题（Gradient Vanishing Problem），这在使用 sigmoid, tanh 等饱和激活函数时尤为严重；</li>
<li>神经网络进行误差反向传播时，<strong>各层都要乘以激活函数的一阶导数</strong>，梯度每传递一层都要衰减一次，网络层数较多时，梯度会不停衰减直至消失；这使得训练时收敛速度非常慢；</li>
<li>使用 ReLU 这类非饱和激活函数训练收敛速度会快很多；</li>
<li>Caffe 中激活函数相关的 <code>Layer</code> 在 <code>include/caffe/neural_layers.hpp</code> 中，可以称为<strong>非线性层</strong>；</li>
<li>非线性层的共同特点就是对前一层 <code>blob</code> 中的数值逐一进行非线性变换，再放回原 <code>blob</code> 中。</li>
</ul>
<hr />
<h2 id="caffe_3">Caffe 中的数据结构</h2>
<p>（TODO: ...）</p>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2019 Yilin Gui.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2019-07-12 01:31:07</p>
      </span>
    </div>

    
    
  </body>
</html>
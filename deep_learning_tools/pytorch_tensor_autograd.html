<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>Pytorch Tensor and Autograd - Yilin's Wiki</title>
    <meta name="keywords" content=""/>
    <meta name="description" content=""/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          <!--$表示行内元素，$$表示块状元素 -->
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!--Mermaid流程图-->
    <script src="https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#deep_learning_tools">deep_learning_tools</a>&nbsp;&#187;&nbsp;Pytorch Tensor and Autograd
    <span class="updated">Page Updated&nbsp;
      2019-07-09 01:01
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">Pytorch Tensor and Autograd</div>

  <h2 id="tensor">Tensor</h2>
<p>在PyTorch中，<code>torch.Tensor</code> 是存储和变换数据的主要工具。 Tensor 和NumPy的多维数组非常类似。然而，Tensor 提供 GPU 计算和自动求梯度等更多功能，使其更加适合深度学习。 </p>
<div class="hlcode"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># tensor([[4.1497e-08, 2.6804e-09, 1.2541e+16, 1.2849e+31],</span>
<span class="c1">#         [1.8395e+25, 6.1963e-04, 3.2907e+21, 8.1934e-10],</span>
<span class="c1">#         [2.1744e+23, 2.6730e-06, 1.7079e-07, 4.2468e-08]])</span>

<span class="c1"># 随机初始化</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># 创建一个 5x3 的 long 型全 0 的 Tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="c1"># 直接根据数据创建</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># 通过现有的Tensor来创建，此方法会默认重用输入Tensor的一些属性，例如数据类型，除非自定义数据类型。</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>  <span class="c1"># 返回的tensor默认具有相同的torch.dtype和torch.device</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="c1"># 指定新的数据类型</span>

<span class="c1"># 可以通过shape或者size()来获取Tensor的形状:</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<p>Pytorch 算术操作</p>
<div class="hlcode"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="c1"># inplace 形式</span>
<span class="n">y</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Pytorch 的大部分操作都用 inplace 形式，用后缀 `_` 标识，如</span>
<span class="n">x</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 将 y 的值拷贝到 x 中</span>
<span class="n">x</span><span class="o">.</span><span class="n">t_</span><span class="p">()</span>  <span class="c1"># 将 x 转置并赋值</span>
</pre></div>


<p>Pytorch tensor 的索引</p>
<p>可以使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是： <strong>索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改</strong> 。</p>
<div class="hlcode"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">y</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>


<p>用 <code>view()</code> 来改变 <code>Tensor</code> 的形状：</p>
<div class="hlcode"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>


<p>注意view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)</p>
<p>所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view。</p>
<div class="hlcode"><pre><span></span><span class="n">x_cp</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
<span class="n">x</span> <span class="o">-=</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_cp</span><span class="p">)</span>
</pre></div>


<p>使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。</p>
<p>另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number：</p>
<div class="hlcode"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>


<p>广播机制</p>
<p>当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。</p>
<p>Tensor 和 NumPy 相互转换</p>
<p>我们很容易用 <code>numpy()</code> 和 <code>from_numpy()</code> 将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！</p>
<p>数据转到 GPU 上，用 <code>to</code></p>
<div class="hlcode"><pre><span></span><span class="c1"># 以下代码只有在PyTorch GPU版本上才会执行</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>          <span class="c1"># GPU</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># 直接创建一个在GPU上的Tensor</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                       <span class="c1"># 等价于 .to(&quot;cuda&quot;)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">))</span>       <span class="c1"># to()还可以同时更改数据类型</span>
</pre></div>


<hr />
<h2 id="autograd">Autograd</h2>
<p>Pytorch 中的 autograd。</p>
<p>Pytorch 中的 Tensor 支持自动求导。</p>
<p>如果 Tensor 的属性 <code>.requires_grad</code> 设置为 True ，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。</p>
<p>完成计算后，可以调用 <code>.backward()</code> 来完成所有梯度计算。此 Tensor 的梯度将累积到 <code>.grad</code> 属性中。</p>
<p>注意在 <code>y.backward()</code> 时，如果 <code>y</code> 是标量，则不需要为 <code>backward()</code> 传入任何参数；否则，需要传入一个与 <code>y</code> 同形的 Tensor。</p>
<p>如果不想要被继续追踪，可以调用 <code>.detach()</code> 将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用 <code>with torch.no_grad()</code> 将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（ <code>requires_grad=True</code> ）的梯度。</p>
<p><code>Function</code> 是另外一个很重要的类。Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个Tensor都有一个 <code>.grad_fn</code> 属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。</p>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2020 Yilin Gui.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2020-02-06 14:40:45</p>
      </span>
    </div>

    
    
  </body>
</html>
<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>自动驾驶技术概览 - Yilin's Wiki</title>
    <meta name="keywords" content=""/>
    <meta name="description" content=""/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          <!--$表示行内元素，$$表示块状元素 -->
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!--Mermaid流程图-->
    <script src="https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#autonomous_driving">autonomous_driving</a>&nbsp;&#187;&nbsp;自动驾驶技术概览
    <span class="updated">Page Updated&nbsp;
      2019-07-27 00:52
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">自动驾驶技术概览</div>

  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#darpa">无人车的起源：DARPA</a></li>
<li><a href="#_1">自动驾驶分级</a></li>
<li><a href="#_2">自动驾驶中的传感器</a><ul>
<li><a href="#lidar">激光雷达 (LIDAR)</a></li>
<li><a href="#camera">摄像头 (Camera)</a></li>
<li><a href="#radar">Radar 等其他传感器</a></li>
<li><a href="#_3">多传感器融合</a></li>
</ul>
</li>
<li><a href="#_4">自动驾驶计算平台</a></li>
<li><a href="#_5">无人车成本</a></li>
<li><a href="#_6">自动驾驶技术拆解</a><ul>
<li><a href="#_7">宏观视角</a><ul>
<li><a href="#_8">感知</a></li>
<li><a href="#_9">高精地图</a></li>
<li><a href="#_10">路径规划</a></li>
<li><a href="#v2x">V2X</a></li>
<li><a href="#_11">定位</a></li>
<li><a href="#_12">规划与决策</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<blockquote>
<p>自动驾驶汽车（AutomatedVehicle；Intelligent Vehicle；Autonomous Vehicle；Self-driving Car；Driverless Car）又称智能汽车、自主汽车、自动驾驶汽车或轮式移动机器人，是一种通过计算机实现自动驾驶的智能汽车。</p>
</blockquote>
<h2 id="darpa">无人车的起源：DARPA</h2>
<p>美国“神盾局”：美国国防高级研究计划局（Defense Advanced Research Projects Agency）。主攻“黑科技”的预研。</p>
<p>DARPA 部分成果：</p>
<ul>
<li>因特网（ARPANET, 1975）</li>
<li>卫星定位（Transit, 1960）</li>
</ul>
<p>DARPA超级挑战赛-2004</p>
<ul>
<li>240km荒漠行车，无人完成全程，最好的队伍只开了不到12km </li>
</ul>
<p>DARPA超级挑战赛-2005 </p>
<ul>
<li>212km荒漠行车，5支队伍完成全程，23支队伍超越2004最远距离 </li>
</ul>
<p>DARPA城市挑战赛-2007 </p>
<ul>
<li>96km城市路段，6支队伍完成全程，第一名仅用时4小时10分</li>
<li>3D激光雷达初露头角，大幅提高了自动驾驶的性能 </li>
<li>自动驾驶开始从理论预研转入应用，当年的参赛队员成为当前的自动驾驶行业中坚，DARPA预研重心转向自行走机器人</li>
</ul>
<h2 id="_1">自动驾驶分级</h2>
<p>NHTSA: 美国高速路安全管理局</p>
<p>SAE: 美国汽车工程师协会</p>
<p>NHTSA 的 L4 对应着 SAE 里的 L4+L5</p>
<p>目前常用 SAE 的 L0~L5 分级。</p>
<h2 id="_2">自动驾驶中的传感器</h2>
<h3 id="lidar">激光雷达 (LIDAR)</h3>
<p>3D 激光雷达特点：</p>
<ul>
<li>大角度或 360 度全景扫描，可直接构建周围环境的三维深度信息</li>
<li>可在各种光照条件下使用（包括夜间） </li>
<li>大致可分为机械旋转扫描和固态扫描两种类型</li>
</ul>
<p>3D 激光雷达缺点：</p>
<ul>
<li>分辨率受限，通常无法获得对象的颜色信息</li>
<li>雪、雾天气会使感知距离下降，雨水烟雾会导致检测盲区</li>
<li>（机械旋转式）激光雷达结构精密，量产困难，可靠性待提升</li>
</ul>
<h3 id="camera">摄像头 (Camera)</h3>
<p>制造技术成熟，成本低廉。</p>
<p>单目摄像头：</p>
<ul>
<li>丰富的颜色、纹理信息，符合人类视觉感知特点</li>
<li>由于二维图像映射三维的误差，单目测距精度有限</li>
<li>视野取决于镜头：长焦看得远，但视野窄；短焦视野广，但看不远</li>
<li>雨、雪、雾天气和镜头有污点时识别性能会下降</li>
<li>除红外摄像头外，不能在夜间使用</li>
</ul>
<p>双目摄像头：</p>
<ul>
<li>通过双个摄像头之间的图像差异来构建三维信息（双目视觉技术）</li>
<li>有效视角比单目摄像头窄</li>
<li>有效视距低于激光雷达（其视距取决于两个摄像头之间的距离(基线)：距离小时远处的场景立体感太小，距离大时又难于获取近处的三维场景）</li>
</ul>
<p>多目摄像头：</p>
<ul>
<li>多目摄像头，可以通过不同的摄像头来覆盖不同范围的场景，既解决了摄像头无法来回切换焦距的问题，也可以一次性解决不同距离下识别清晰度的问题。</li>
<li>e.g. 广角镜头用来看近处的环境，80度的覆盖30米左右的环境，60度覆盖中远距离，40度负责远距离观察。不同的摄像头负责观察不同距离、角度范围的场景，各司其职，互不干扰。</li>
<li>难点：安装方案，算法配合，计算开销</li>
</ul>
<h3 id="radar">Radar 等其他传感器</h3>
<p>毫米波雷达：</p>
<ul>
<li>性价比高，车规级产品量产容易</li>
<li>探测距离远于激光雷达</li>
<li>缺点：数据稳定性差，毫米波发出的电磁波对金属极为敏感</li>
</ul>
<p>超声波雷达：</p>
<ul>
<li>常见的倒车雷达就是超声波雷达</li>
<li>通常作用范围3米以内，角度分辨率很低</li>
<li>通常只能给出探测距离，无法精确描述障碍物的位置</li>
</ul>
<p>2D 激光雷达：</p>
<ul>
<li>通常可扫描1-4个平面，水平分辨率很高</li>
</ul>
<h3 id="_3">多传感器融合</h3>
<ul>
<li>单一传感器无法覆盖全部测量场景需求<ul>
<li>远/近距离使用，贴地物体检测，速度测量，测量分辨率</li>
<li>雨、雪、雾天气，夜间使用</li>
</ul>
</li>
<li>基于单一传感器的识别性能不佳<ul>
<li>摄像头分割/检测三维物体难，激光雷达单帧分辨率低</li>
</ul>
</li>
<li>多传感器融合是趋势，利用多个传感器信息融合，冗余、互补，提高综合感知能力</li>
</ul>
<h2 id="_4">自动驾驶计算平台</h2>
<p>NVIDIA Drive PX2</p>
<ul>
<li>全自动驾驶计算平台</li>
<li>12个最高性能64位ARM核心</li>
<li>2个最新桌面级GPU核</li>
<li>最新的16 nm工艺 </li>
<li>250W功耗，需要专门的液态冷却系统来完成散热</li>
<li>相当于 150 个 Macbook pro （2016年）</li>
</ul>
<p>Mobileye EyeQ3</p>
<ul>
<li>ADAS 计算平台</li>
<li>4个MIPS CPU 核心，性能大约相当于2010年左右的ARM内核</li>
<li>4个运算加速核，总处理能力大致相当于1台MacBook</li>
<li>2.5W功耗</li>
</ul>
<p>Mobileye EyeQ4</p>
<ul>
<li>ADAS 计算平台</li>
<li>14 个计算核心，其中10个为特制矢量加速器，大幅提升了视觉处理和数据解读的性能</li>
<li>四个CPU处理器内核，每个内核又拥有四个硬件线程，性能超过EyeQ2和EyeQ3使用的创新型向量微码六核处理器（VMP）</li>
</ul>
<p>Mobileye EyeQ5 （2020）</p>
<ul>
<li>超低功耗，对标全自动驾驶计算平台</li>
</ul>
<h2 id="_5">无人车成本</h2>
<p>2016 年数据</p>
<p>激光雷达</p>
<ul>
<li>未来预期价格：\$90-\$8000</li>
<li>价格波动因素：2D/3D，机械扫描/固态扫描等不同技术方向是价格差异的主要原因</li>
</ul>
<p>摄像头</p>
<ul>
<li>单目：\$125-\$150</li>
<li>双目：\$150-\$200</li>
</ul>
<p>毫米波雷达</p>
<ul>
<li>长距：\$125-\$150</li>
<li>短距：\$50-\$100</li>
</ul>
<p>车载计算平台</p>
<ul>
<li>传感器总成本的 50% -- 200%</li>
</ul>
<p>超声波雷达</p>
<ul>
<li>\$15-\$20</li>
</ul>
<p>里程计</p>
<ul>
<li>\$80-\$120</li>
</ul>
<p>DGPS</p>
<ul>
<li>\$80 -- \$6000</li>
<li>价格波动因素：DGPS技术（伪距差分/RTK），是否集成惯性导航单元IMU</li>
</ul>
<hr />
<h2 id="_6">自动驾驶技术拆解</h2>
<h3 id="_7">宏观视角</h3>
<div class="hlcode"><pre><span class="err">感知外界环境</span> <span class="o">-&gt;</span> <span class="err">映射汽车视角</span> <span class="o">-&gt;</span> <span class="err">驾驶汽车决策</span>
</pre></div>


<ul>
<li>感知外界环境<ul>
<li>路线规划：我要去哪，哪里能走</li>
<li>地图：路在哪里，限速吗</li>
<li>车载感知：附近有什么障碍物</li>
</ul>
</li>
<li>映射汽车视角<ul>
<li>车观视角：无人车看到的环境是什么样的</li>
<li>本车定位：无人车自身在什么地方</li>
</ul>
</li>
<li>驾驶汽车决策<ul>
<li>行车策略</li>
<li>轨迹规划</li>
<li>轨迹执行</li>
</ul>
</li>
</ul>
<h4 id="_8">感知</h4>
<p>感知技术的三个层面（由低到高） </p>
<ul>
<li>能避让行车方向的障碍物：碰撞预警（半自动驾驶） </li>
<li>能在空无一人的道路上开车：车道位置/交通信息牌/红绿灯感知</li>
<li>能在有行人车辆的道路上开车：运动物体的感知</li>
</ul>
<p>感知技术难点</p>
<ul>
<li>交通场景中物体类别繁多，检测难</li>
<li>同时跟踪场景中多个障碍物，需要保证实时性</li>
<li>训练数据收集、标注难</li>
<li>考虑传感器成本，技术方案选择</li>
</ul>
<h4 id="_9">高精地图</h4>
<p>自动驾驶中使用高精地图的意义</p>
<ul>
<li>为高精定位提供参照物坐标 <ul>
<li>在地图中预先标记特定物体（如电线杆）的精确坐标，通过测算车辆与其之间的距离来推算本车坐标 </li>
</ul>
</li>
<li>记录已知环境，提升环境感知的总体准确率 <ul>
<li>高精地图对静态环境（车道线、护栏、路牌信息等）提供高可靠性描述，可降低现场识别的难度，提升总体准确率性 </li>
</ul>
</li>
<li>归一索引行车记录，作为大数据训练的基础 <ul>
<li>为持续训练自动驾驶算法，需要对不同车次的实际行车数据（本车轨迹、其它车辆行人的运动等）进行归一化记录 </li>
<li>在高精地图建立的三维空间上映射车辆位置，是对齐不同车次数据记录的最佳方式，精确度最高</li>
</ul>
</li>
</ul>
<h4 id="_10">路径规划</h4>
<p>路线规划是已经成熟的地图技术，例子：百度地图</p>
<h4 id="v2x">V2X</h4>
<p>Vehicle To Everything</p>
<ul>
<li>车车通信（V2V）和车与基础设施通讯（V2I）的总称</li>
<li>例子<ul>
<li>向车辆报告前方拥堵状况；向调度中心报告本车位置</li>
<li>向车辆报告前方红绿灯状态</li>
<li>通过第三方后台，汇总道路上所有车辆的实时位置</li>
</ul>
</li>
<li>在基础设施建设完备前，V2X 无法可靠地帮助自动驾驶</li>
</ul>
<h4 id="_11">定位</h4>
<p>两种定位方式：全局定位和相对定位。</p>
<p>全局定位</p>
<ul>
<li>DGPS(Differential-GPS)：通过地面基站信号校正GPS精度，在不依赖地图信息的情况下获取车辆绝对坐标</li>
<li>IMU(Inertial Measurement Unit)：记录车辆的加速度、角速度信息，推算其距离上次精准定位后的位移，作为DGPS的补充</li>
</ul>
<p>相对定位</p>
<ul>
<li>SLAM(Simultaneous  Localization and Mapping): 测量车辆与多个已知坐标的参照物之间的距离，从而计算出车辆的坐标</li>
</ul>
<p>稳定的定位输出需要两种定位技术的融合。</p>
<p>全局定位技术无法提供精度稳定的输出 </p>
<ul>
<li>DGPS信号接收易受干扰，不稳定 </li>
<li>IMU只能缓解DGPS的不稳定问题，但无法根除</li>
</ul>
<p>相对定位+全局定位的技术被业界广泛使用。</p>
<h4 id="_12">规划与决策</h4>
<p>车辆驾驶可分为行车策略、轨迹规划、轨迹执行三个步骤</p>
<ul>
<li>行车策略(Maneuver Planning)决定汽车将要采取的行动<ul>
<li>直行跟随还是并线超车？是否要转弯？ </li>
</ul>
</li>
<li>轨迹规划(Trajectory Planning)为汽车的行动规划合适的轨迹<ul>
<li>轨迹尽量平滑，尽量远离所有障碍物，高速时不应急转弯等</li>
</ul>
</li>
<li>轨迹执行(Trajectory Execution)控制实际轨迹<ul>
<li>尽量减小实际轨迹与规划轨迹间的误差</li>
</ul>
</li>
</ul>
<p>行车策略是自动驾驶的核心</p>
<ul>
<li>是否具有行车策略能力，是全自动驾驶与半自动驾驶的主要区别</li>
<li>良好的车载感知性能，是开展行车策略研究的前提</li>
<li>车辆/行人的运动预测（Motion Planning），选取什么模型（RL，有监督？）</li>
<li>道路行驶场景的归纳和穷举（路测+仿真模拟）</li>
</ul>
<p>轨迹规划是一个几何曲线的动态规划问题</p>
<ul>
<li>选择合理的几何模型，生成一组平滑的备选轨迹<ul>
<li>贝塞尔函数、回旋曲线函数、纳尔逊多项式、样条函数等</li>
</ul>
</li>
<li>建立代价函数(cost function)，优选轨迹<ul>
<li>代价函数要素：障碍物位置、运动趋势、车辆操控性能</li>
</ul>
</li>
<li>轨迹规划输出要素<ul>
<li>轨迹曲线，车身姿态、转向角、转向角速度，是否需要加减速</li>
</ul>
</li>
</ul>
<p>轨迹执行</p>
<ul>
<li>需要良好的控制回路：位置、速度、加速度多环反馈+前馈</li>
<li>精确、实时的车辆状态监测</li>
</ul>
<p>现阶段，感知算法是自动驾驶的最大瓶颈，其次是行车策略。</p>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2019 Yilin Gui.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2019-11-14 01:54:04</p>
      </span>
    </div>

    
    
  </body>
</html>
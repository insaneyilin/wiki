<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>Dive Into Deep Learning notes(3) -- 计算机视觉与目标检测 - Yilin's Wiki</title>
    <meta name="keywords" content=""/>
    <meta name="description" content=""/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          <!--$表示行内元素，$$表示块状元素 -->
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!--Mermaid流程图-->
    <script src="https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#course_notes">course_notes</a>&nbsp;&#187;&nbsp;Dive Into Deep Learning notes(3) -- 计算机视觉与目标检测
    <span class="updated">Page Updated&nbsp;
      2019-10-09 23:34
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">Dive Into Deep Learning notes(3) -- 计算机视觉与目标检测</div>

  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#ch5">ch5 计算机视觉</a><ul>
<li><a href="#image-augmentation">图像数据增强（Image Augmentation）</a></li>
<li><a href="#finetune">训练微调（Finetune）</a></li>
<li><a href="#_1">目标检测</a><ul>
<li><a href="#anchor-box">Anchor Box （锚框）</a><ul>
<li><a href="#anchor-box_1">生成多个 anchor box</a></li>
</ul>
</li>
<li><a href="#iou">IOU</a></li>
<li><a href="#_2">贪心指派法</a></li>
<li><a href="#nms">非极大值抑制 NMS</a></li>
<li><a href="#_3">多尺度目标检测</a></li>
</ul>
</li>
<li><a href="#ssd">单发多框目标检测（SSD）</a></li>
</ul>
</li>
</ul>
</div>
<p><a href="http://zh.d2l.ai/index.html">动手学深度学习</a></p>
<hr />
<h1 id="ch5">ch5 计算机视觉</h1>
<h2 id="image-augmentation">图像数据增强（Image Augmentation）</h2>
<p>图像增广（image augmentation）技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。我们也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。可以说，在当年AlexNet的成功中，图像增广技术功不可没。</p>
<p>左右翻转图像通常不改变物体的类别。它是最早也是最广泛使用的一种图像增广方法。</p>
<p>上下翻转不如左右翻转通用，对于某些图像，上下翻转不会造成识别障碍。</p>
<p>可以通过对图像随机裁剪来让物体以不同的比例出现在图像的不同位置，这同样能够降低模型对目标位置的敏感性。</p>
<p>另一类增广方法是变化颜色。我们可以从4个方面改变图像的颜色：亮度、对比度、饱和度和色调。</p>
<p>实际应用中我们会将多个图像增广方法叠加使用。<br />
augmentation 也是一种 regularization，避免过拟合。</p>
<h2 id="finetune">训练微调（Finetune）</h2>
<p>迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。</p>
<p>微调（fine tuning）</p>
<ol>
<li>在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。</li>
<li>创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。</li>
<li>为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。</li>
</ol>
<p>当目标数据集远远小于源数据集时，微调有助于提升模型的泛化能力。</p>
<p><img alt="" src="/wiki/attach/images/d2lzh_notes/figure_9.1.png" /></p>
<ul>
<li>迁移学习将从源数据集学到的知识迁移到目标数据集上。微调是迁移学习的一种常用技术。</li>
<li>目标模型复制了源模型上除了输出层外的所有模型设计及其参数，并基于目标数据集微调这些参数。而目标模型的输出层需要从头训练。</li>
<li>一般来说，微调参数会使用较小的学习率，而从头训练输出层可以使用较大的学习率。</li>
</ul>
<h2 id="_1">目标检测</h2>
<p>在图像分类任务里，我们假设图像里只有一个主体目标，并关注如何识别该目标的类别。然而，很多时候图像里有多个我们感兴趣的目标，我们 <strong>不仅想知道它们的类别，还想得到它们在图像中的具体位置</strong> 。在计算机视觉里，我们将这类任务称为目标检测（object detection）或物体检测。</p>
<p>在目标检测里，我们通常使用边界框（bounding box）来描述目标位置。边界框是一个矩形框，可以由矩形左上角的x和y轴坐标与右下角的x和y轴坐标确定。我们根据上面的图的坐标信息来定义图中狗和猫的边界框。图中的坐标原点在图像的左上角，原点往右和往下分别为x轴和y轴的正方向。</p>
<h3 id="anchor-box">Anchor Box （锚框）</h3>
<p>目标检测算法通常会在输入图像中 <strong>采样大量的区域</strong> ，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边缘从而更准确地预测目标的真实边界框（ground-truth bounding box）。不同的模型使用的区域采样方法可能不同。这里我们介绍其中的一种方法： <strong>它以每个像素为中心生成多个大小和宽高比（aspect ratio）不同的边界框</strong> 。这些边界框被称为锚框（anchor box）。我们将在后面基于锚框实践目标检测。（one stage 方法）</p>
<h4 id="anchor-box_1">生成多个 anchor box</h4>
<p>假设输入图像高为 h ，宽为 w 。我们分别以图像的每个像素为中心生成不同形状的锚框。设大小为 <code>s∈(0,1]</code>， 且宽高比为 <code>r&gt;0</code> ，那么锚框的宽和高将分别为 <code>ws√r</code> 和 <code>hs/√r</code> 。当中心位置给定时，已知宽和高的锚框是确定的。</p>
<p>下面我们分别设定好一组大小 <code>s1,…,sn</code> 和一组宽高比 <code>r1,…,rm</code> 。如果以每个像素为中心时使用所有的大小与宽高比的组合，输入图像将一共得到 <code>whnm</code> 个锚框。虽然这些锚框可能覆盖了所有的真实边界框，但计算复杂度容易过高。因此，我们通常只对包含 <code>s1</code> 或 <code>r1</code> 的大小与宽高比的组合感兴趣，即</p>
<p><code>(s1,r1),(s1,r2),…,(s1,rm),(s2,r1),(s3,r1),…,(sn,r1).</code></p>
<p>也就是说，以相同像素为中心的锚框的数量为 <code>n+m−1</code> 。对于整个输入图像，我们将一共生成 <code>wh(n+m−1)</code> 个锚框。</p>
<h3 id="iou">IOU</h3>
<p>交并比，即 IOU，即 Jaccard Index。我们使用交并比来衡量锚框与真实边界框以及锚框与锚框之间的相似度。</p>
<p>在训练集中，我们 <strong>将每个锚框视为一个训练样本</strong> 。为了训练目标检测模型，我们需要为每个锚框标注两类标签： <strong>一是锚框所含目标的类别，简称类别；二是真实边界框相对锚框的偏移量，简称偏移量（offset）</strong>。在目标检测时，我们首先生成多个锚框，然后为每个锚框预测类别以及偏移量，接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框。</p>
<p>我们知道，在目标检测的训练集中，每个图像已标注了真实边界框的位置以及所含目标的类别。在生成锚框之后，我们主要依据与锚框相似的真实边界框的位置和类别信息为锚框标注。那么，该如何为锚框分配与其相似的真实边界框呢？</p>
<h3 id="_2">贪心指派法</h3>
<p><img alt="" src="/wiki/attach/images/d2lzh_notes/obj_det_greedy_assign.png" /></p>
<p><img alt="" src="/wiki/attach/images/d2lzh_notes/figure_9.3.png" /></p>
<p><img alt="" src="/wiki/attach/images/d2lzh_notes/obj_det_greedy_assign2.png" /></p>
<h3 id="nms">非极大值抑制 NMS</h3>
<p>在模型预测阶段，我们先为图像生成多个锚框，并为这些锚框一一预测类别和偏移量。随后，我们根据锚框及其预测偏移量得到预测边界框。当锚框数量较多时，同一个目标上可能会输出较多相似的预测边界框。为了使结果更加简洁，我们可以移除相似的预测边界框。常用的方法叫作非极大值抑制（non-maximum suppression，NMS）。</p>
<p><img alt="" src="/wiki/attach/images/d2lzh_notes/objdet_nms.png" /></p>
<h3 id="_3">多尺度目标检测</h3>
<blockquote>
<p>如何解决检测任务中的目标存在的较大的尺度变化（large scale variation）。</p>
</blockquote>
<p>在“锚框”一节中，我们在实验中以输入图像的每个像素为中心生成多个锚框。这些锚框是对输入图像不同区域的采样。然而，如果以图像每个像素为中心都生成锚框，很容易生成过多锚框而造成计算量过大。举个例子，假设输入图像的高和宽分别为561像素和728像素，如果以每个像素为中心生成5个不同形状的锚框，那么一张图像上则需要标注并预测200多万个锚框（561×728×5）。</p>
<p>减少锚框个数并不难。 <strong>一种简单的方法是在输入图像中均匀采样一小部分像素</strong> ，并以采样的像素为中心生成锚框。此外， <strong>在不同尺度下，我们可以生成不同数量和不同大小的锚框</strong> 。值得注意的是， <strong>较小目标比较大目标在图像上出现位置的可能性更多</strong> 。举个简单的例子：形状为1×1、1×2和2×2的目标在形状为2×2的图像上可能出现的位置分别有4、2和1种。因此， <strong>当使用较小锚框来检测较小目标时，我们可以采样较多的区域；而当使用较大锚框来检测较大目标时，我们可以采样较少的区域</strong>。</p>
<ul>
<li>可以在多个尺度下生成不同数量和不同大小的锚框，从而在多个尺度下检测不同大小的目标。</li>
<li>特征图的形状能确定任一图像上均匀采样的锚框中心。</li>
<li>用输入图像在某个感受野区域内的信息来预测输入图像上与该区域相近的锚框的类别和偏移量。</li>
</ul>
<h2 id="ssd">单发多框目标检测（SSD）</h2>
<p>SSD(Single Shot multi-box Detection).</p>
<p><img alt="" src="/wiki/attach/images/d2lzh_notes/figure_9.4.png" /></p>
<p>SSD 主要由一个基础网络块和若干个多尺度特征块串联而成。其中基础网络块用来从原始图像中抽取特征，因此一般会选择常用的深度卷积神经网络。单发多框检测论文中选用了在分类层之前截断的VGG，现在也常用ResNet替代。我们可以设计基础网络，使它输出的高和宽较大。这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。如此一来，图9.4中越靠近顶部的多尺度特征块输出的特征图越小，故而基于特征图生成的锚框也越少，加之特征图中每个单元感受野越大，因此更适合检测尺寸较大的目标。由于单发多框检测基于基础网络块和各个多尺度特征块生成不同数量和不同大小的锚框，并通过预测锚框的类别和偏移量（即预测边界框）检测不同大小的目标，因此 <strong>单发多框检测是一个多尺度的目标检测模型</strong> 。</p>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2020 Yilin Gui.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2020-05-23 02:01:15</p>
      </span>
    </div>

    
    
  </body>
</html>